{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis\n",
    "import regex as re\n",
    "import contractions\n",
    "import num2words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2d6b1bd0134aa5806cd4a82e29481d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\n",
    "    \"./data/nlp-getting-started/train.csv\",)\n",
    "\n",
    "# Encoding target variable\n",
    "#data[\"target\"] = np.where(data[\"target\"] == \"spam\", 1, 0)\n",
    "\n",
    "df = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling null values with some number\n",
    "\n",
    "df['keyword'].fillna('', inplace=True)\n",
    "df['location'].fillna('',inplace=True)\n",
    "\n",
    "# dropping the duplicates\n",
    "df.drop_duplicates(subset ='text',keep=False, inplace=True)\n",
    "\n",
    "\n",
    "# converting to strings\n",
    "\n",
    "df['keyword'] = df['keyword'].astype('object')\n",
    "df['location'] = df['location'].astype('object')\n",
    "df['text'] = df['text'].astype('str')\n",
    "\n",
    "# adding the columns keyword and text\n",
    "df['text_final'] = df['keyword'] + ' ' + df['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contractions\n",
    "df['text_final_1'] = df['text_final'].apply(lambda x: [contractions.fix(word) for word in x.split(' ')])\n",
    "\n",
    "#joining back the list of items into one string\n",
    "df['text_final_1'] = [' '.join(map(str, l)) for l in df['text_final_1']]\n",
    "\n",
    "# Noise Cleaning - spacing, special characters, lowercasing \n",
    "\n",
    "df['text_final_1'] = df['text_final_1'].str.lower()\n",
    "df['text_final_1'] = df['text_final_1'].apply(lambda x: re.sub(r'[^\\w\\d\\s\\']+', '', x))\n",
    "df['text_final_2'] = df['text_final_1'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "\n",
    "#nltk tokenization\n",
    "\n",
    "df['text_final_1'] = df['text_final_1'].apply(word_tokenize)\n",
    "df['text_final_2'] = df['text_final_2'].apply(word_tokenize)\n",
    "\n",
    "# remove stop words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['text_final_1'] = df['text_final_1'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df['text_final_1'] = [' '.join(map(str, l)) for l in df['text_final_1']]\n",
    "\n",
    "df['text_final_2'] = df['text_final_2'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df['text_final_2'] = [' '.join(map(str, l)) for l in df['text_final_2']]\n",
    "\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "df['text_final_url'] = df['text_final_2'].apply(lambda x: remove_url(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_final_1</th>\n",
       "      <th>text_final_2</th>\n",
       "      <th>text_final_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>7529</td>\n",
       "      <td>oil%20spill</td>\n",
       "      <td></td>\n",
       "      <td>@TroySlaby22 slicker than an oil spill</td>\n",
       "      <td>0</td>\n",
       "      <td>oil%20spill @TroySlaby22 slicker than an oil s...</td>\n",
       "      <td>oil20spill troyslaby22 slicker oil spill</td>\n",
       "      <td>oil spill troyslaby slicker oil spill</td>\n",
       "      <td>oil spill troyslaby slicker oil spill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6775</th>\n",
       "      <td>9707</td>\n",
       "      <td>tornado</td>\n",
       "      <td></td>\n",
       "      <td>Pretty teen Hayden Ryan poses and strips out o...</td>\n",
       "      <td>0</td>\n",
       "      <td>tornado Pretty teen Hayden Ryan poses and stri...</td>\n",
       "      <td>tornado pretty teen hayden ryan poses strips p...</td>\n",
       "      <td>tornado pretty teen hayden ryan poses strips p...</td>\n",
       "      <td>tornado pretty teen hayden ryan poses strips p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6352</th>\n",
       "      <td>9082</td>\n",
       "      <td>structural%20failure</td>\n",
       "      <td></td>\n",
       "      <td>Slums are a manifestation state failure to pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>structural%20failure Slums are a manifestation...</td>\n",
       "      <td>structural20failure slums manifestation state ...</td>\n",
       "      <td>structural failure slums manifestation state f...</td>\n",
       "      <td>structural failure slums manifestation state f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4487</th>\n",
       "      <td>6383</td>\n",
       "      <td>hostages</td>\n",
       "      <td>The Universe</td>\n",
       "      <td>Cont'd- #Sinjar: referring to a 40-pg document...</td>\n",
       "      <td>1</td>\n",
       "      <td>hostages Cont'd- #Sinjar: referring to a 40-pg...</td>\n",
       "      <td>hostages cont 'd sinjar referring 40pg documen...</td>\n",
       "      <td>hostages cont sinjar referring pg document gro...</td>\n",
       "      <td>hostages cont sinjar referring pg document gro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               keyword      location  \\\n",
       "5267  7529           oil%20spill                 \n",
       "6775  9707               tornado                 \n",
       "6352  9082  structural%20failure                 \n",
       "4487  6383              hostages  The Universe   \n",
       "\n",
       "                                                   text  target  \\\n",
       "5267             @TroySlaby22 slicker than an oil spill       0   \n",
       "6775  Pretty teen Hayden Ryan poses and strips out o...       0   \n",
       "6352  Slums are a manifestation state failure to pro...       1   \n",
       "4487  Cont'd- #Sinjar: referring to a 40-pg document...       1   \n",
       "\n",
       "                                             text_final  \\\n",
       "5267  oil%20spill @TroySlaby22 slicker than an oil s...   \n",
       "6775  tornado Pretty teen Hayden Ryan poses and stri...   \n",
       "6352  structural%20failure Slums are a manifestation...   \n",
       "4487  hostages Cont'd- #Sinjar: referring to a 40-pg...   \n",
       "\n",
       "                                           text_final_1  \\\n",
       "5267           oil20spill troyslaby22 slicker oil spill   \n",
       "6775  tornado pretty teen hayden ryan poses strips p...   \n",
       "6352  structural20failure slums manifestation state ...   \n",
       "4487  hostages cont 'd sinjar referring 40pg documen...   \n",
       "\n",
       "                                           text_final_2  \\\n",
       "5267              oil spill troyslaby slicker oil spill   \n",
       "6775  tornado pretty teen hayden ryan poses strips p...   \n",
       "6352  structural failure slums manifestation state f...   \n",
       "4487  hostages cont sinjar referring pg document gro...   \n",
       "\n",
       "                                         text_final_url  \n",
       "5267              oil spill troyslaby slicker oil spill  \n",
       "6775  tornado pretty teen hayden ryan poses strips p...  \n",
       "6352  structural failure slums manifestation state f...  \n",
       "4487  hostages cont sinjar referring pg document gro...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['text_final_2'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7434, 51)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7434, 51)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset between train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.794\n",
      "accuracy = 0.807\n",
      "precision = 0.812\n",
      "recall = 0.707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"AUC = {:.3f}\".format(roc_auc_score(y_test, predictions)))\n",
    "print(\"accuracy = {:.3f}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"precision = {:.3f}\".format(precision_score(y_test, predictions)))\n",
    "print(\"recall = {:.3f}\".format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074233458848843"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de2b2c42f19cdb5d2f24461361f61e9d985b64a4ca4a8f91e8c83c5a49062fa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
