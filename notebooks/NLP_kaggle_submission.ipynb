{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Challenge Submission file \n",
    "\n",
    "<br>\n",
    "\n",
    "- ## **_word embeddings LogisticRegression_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/xuxu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xuxu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/xuxu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/xuxu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/xuxu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url = re.compile(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emotions\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji.sub(r'', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punc = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"text_clean\"] = df_test[\"text\"].apply(lambda x: remove_url(x))\n",
    "df_test[\"text_clean\"] = df_test[\"text_clean\"].apply(lambda x: remove_emoji(x))\n",
    "df_test[\"text_clean\"] = df_test[\"text_clean\"].apply(lambda x: remove_html(x))\n",
    "df_test[\"text_clean\"] = df_test[\"text_clean\"].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"text_token\"] = df_test[\"text_clean\"].apply(word_tokenize)\n",
    "df_test[\"text_token\"] = df_test[\"text_token\"].apply(lambda x: [word.lower() for word in x])\n",
    "df_test[\"text_final\"] = df_test[\"text_token\"].apply(lambda x: [word for word in x if word not in stopword])\n",
    "df_test[\"pos_tags\"] = df_test[\"text_final\"].apply(nltk.tag.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wordnet(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "df_test[\"wordnet_tag\"] = df_test[\"pos_tags\"].apply(lambda x: [(word, convert_to_wordnet(pos_tag)) for (word, pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "df_test[\"lemmatize\"] = df_test[\"wordnet_tag\"].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "df_test[\"lemmatize\"] = df_test[\"lemmatize\"].apply(lambda x: [word for word in x if word not in stopword])\n",
    "df_test[\"text_lemma\"] = [' '.join(map(str, x)) for x in df_test[\"lemmatize\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"char_count1\"] = df_test[\"text_clean\"].apply(len) \n",
    "df_test[\"char_count2\"] = df_test[\"text_lemma\"].apply(len)\n",
    "df_test['word_count1'] = df_test['text_clean'].apply(lambda x: len(str.split(x)))\n",
    "df_test['word_count2'] = df_test['text_lemma'].apply(lambda x: len(str.split(x)))\n",
    "df_test['mword_leng1'] = df_test['text_clean'].str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))\n",
    "df_test['mword_leng2'] = df_test['text_lemma'].str.split().apply(lambda x: [len(i) for i in x]).map(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_final</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_tag</th>\n",
       "      <th>lemmatize</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>char_count1</th>\n",
       "      <th>char_count2</th>\n",
       "      <th>word_count1</th>\n",
       "      <th>word_count2</th>\n",
       "      <th>mword_leng1</th>\n",
       "      <th>mword_leng2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[just, happened, a, terrible, car, crash]</td>\n",
       "      <td>[happened, terrible, car, crash]</td>\n",
       "      <td>[(happened, VBN), (terrible, JJ), (car, NN), (...</td>\n",
       "      <td>[(happened, v), (terrible, a), (car, n), (cras...</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Heard about earthquake is different cities sta...</td>\n",
       "      <td>[heard, about, earthquake, is, different, citi...</td>\n",
       "      <td>[heard, earthquake, different, cities, stay, s...</td>\n",
       "      <td>[(heard, RB), (earthquake, NN), (different, JJ...</td>\n",
       "      <td>[(heard, r), (earthquake, n), (different, a), ...</td>\n",
       "      <td>[heard, earthquake, different, city, stay, saf...</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "      <td>61</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>6.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "      <td>[there, is, a, forest, fire, at, spot, pond, g...</td>\n",
       "      <td>[forest, fire, spot, pond, geese, fleeing, acr...</td>\n",
       "      <td>[(forest, JJS), (fire, NN), (spot, NN), (pond,...</td>\n",
       "      <td>[(forest, a), (fire, n), (spot, n), (pond, n),...</td>\n",
       "      <td>[forest, fire, spot, pond, geese, flee, across...</td>\n",
       "      <td>forest fire spot pond geese flee across street...</td>\n",
       "      <td>94</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>Apocalypse lighting Spokane wildfires</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfires]</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfires]</td>\n",
       "      <td>[(apocalypse, NN), (lighting, VBG), (spokane, ...</td>\n",
       "      <td>[(apocalypse, n), (lighting, v), (spokane, n),...</td>\n",
       "      <td>[apocalypse, light, spokane, wildfire]</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>[typhoon, soudelor, kills, 28, in, china, and,...</td>\n",
       "      <td>[typhoon, soudelor, kills, 28, china, taiwan]</td>\n",
       "      <td>[(typhoon, NN), (soudelor, NN), (kills, VBZ), ...</td>\n",
       "      <td>[(typhoon, n), (soudelor, n), (kills, v), (28,...</td>\n",
       "      <td>[typhoon, soudelor, kill, 28, china, taiwan]</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>Were shakingIts an earthquake</td>\n",
       "      <td>[were, shakingits, an, earthquake]</td>\n",
       "      <td>[shakingits, earthquake]</td>\n",
       "      <td>[(shakingits, NNS), (earthquake, NN)]</td>\n",
       "      <td>[(shakingits, n), (earthquake, n)]</td>\n",
       "      <td>[shakingits, earthquake]</td>\n",
       "      <td>shakingits earthquake</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>Theyd probably still show more life than Arsen...</td>\n",
       "      <td>[theyd, probably, still, show, more, life, tha...</td>\n",
       "      <td>[theyd, probably, still, show, life, arsenal, ...</td>\n",
       "      <td>[(theyd, NN), (probably, RB), (still, RB), (sh...</td>\n",
       "      <td>[(theyd, n), (probably, r), (still, r), (show,...</td>\n",
       "      <td>[theyd, probably, still, show, life, arsenal, ...</td>\n",
       "      <td>theyd probably still show life arsenal yesterd...</td>\n",
       "      <td>68</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>5.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>Hey How are you</td>\n",
       "      <td>[hey, how, are, you]</td>\n",
       "      <td>[hey]</td>\n",
       "      <td>[(hey, NN)]</td>\n",
       "      <td>[(hey, n)]</td>\n",
       "      <td>[hey]</td>\n",
       "      <td>hey</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>What a nice hat</td>\n",
       "      <td>[what, a, nice, hat]</td>\n",
       "      <td>[nice, hat]</td>\n",
       "      <td>[(nice, JJ), (hat, NN)]</td>\n",
       "      <td>[(nice, a), (hat, n)]</td>\n",
       "      <td>[nice, hat]</td>\n",
       "      <td>nice hat</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>Fuck off</td>\n",
       "      <td>[fuck, off]</td>\n",
       "      <td>[fuck]</td>\n",
       "      <td>[(fuck, NN)]</td>\n",
       "      <td>[(fuck, n)]</td>\n",
       "      <td>[fuck]</td>\n",
       "      <td>fuck</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No I don't like cold!</td>\n",
       "      <td>No I dont like cold</td>\n",
       "      <td>[no, i, dont, like, cold]</td>\n",
       "      <td>[dont, like, cold]</td>\n",
       "      <td>[(dont, NN), (like, IN), (cold, NN)]</td>\n",
       "      <td>[(dont, n), (like, n), (cold, n)]</td>\n",
       "      <td>[dont, like, cold]</td>\n",
       "      <td>dont like cold</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOOOOOOOOO! Don't do that!</td>\n",
       "      <td>NOOOOOOOOO Dont do that</td>\n",
       "      <td>[nooooooooo, dont, do, that]</td>\n",
       "      <td>[nooooooooo, dont]</td>\n",
       "      <td>[(nooooooooo, NNS), (dont, NN)]</td>\n",
       "      <td>[(nooooooooo, n), (dont, n)]</td>\n",
       "      <td>[nooooooooo, dont]</td>\n",
       "      <td>nooooooooo dont</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No don't tell me that!</td>\n",
       "      <td>No dont tell me that</td>\n",
       "      <td>[no, dont, tell, me, that]</td>\n",
       "      <td>[dont, tell]</td>\n",
       "      <td>[(dont, NN), (tell, NN)]</td>\n",
       "      <td>[(dont, n), (tell, n)]</td>\n",
       "      <td>[dont, tell]</td>\n",
       "      <td>dont tell</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What if?!</td>\n",
       "      <td>What if</td>\n",
       "      <td>[what, if]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>[awesome]</td>\n",
       "      <td>[awesome]</td>\n",
       "      <td>[(awesome, NN)]</td>\n",
       "      <td>[(awesome, n)]</td>\n",
       "      <td>[awesome]</td>\n",
       "      <td>awesome</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London</td>\n",
       "      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n",
       "      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n",
       "      <td>[birmingham, wholesale, market, is, ablaze, bb...</td>\n",
       "      <td>[birmingham, wholesale, market, ablaze, bbc, n...</td>\n",
       "      <td>[(birmingham, JJ), (wholesale, JJ), (market, N...</td>\n",
       "      <td>[(birmingham, a), (wholesale, a), (market, n),...</td>\n",
       "      <td>[birmingham, wholesale, market, ablaze, bbc, n...</td>\n",
       "      <td>birmingham wholesale market ablaze bbc news fi...</td>\n",
       "      <td>96</td>\n",
       "      <td>83</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>5.785714</td>\n",
       "      <td>6.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Niall's place | SAF 12 SQUAD |</td>\n",
       "      <td>@sunkxssedharry will you wear shorts for race ...</td>\n",
       "      <td>sunkxssedharry will you wear shorts for race a...</td>\n",
       "      <td>[sunkxssedharry, will, you, wear, shorts, for,...</td>\n",
       "      <td>[sunkxssedharry, wear, shorts, race, ablaze]</td>\n",
       "      <td>[(sunkxssedharry, NN), (wear, NN), (shorts, NN...</td>\n",
       "      <td>[(sunkxssedharry, n), (wear, n), (shorts, n), ...</td>\n",
       "      <td>[sunkxssedharry, wear, short, race, ablaze]</td>\n",
       "      <td>sunkxssedharry wear short race ablaze</td>\n",
       "      <td>52</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NIGERIA</td>\n",
       "      <td>#PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...</td>\n",
       "      <td>PreviouslyOnDoyinTv Toke MakinwaÛªs marriage ...</td>\n",
       "      <td>[previouslyondoyintv, toke, makinwaûªs, marri...</td>\n",
       "      <td>[previouslyondoyintv, toke, makinwaûªs, marri...</td>\n",
       "      <td>[(previouslyondoyintv, NN), (toke, VBD), (maki...</td>\n",
       "      <td>[(previouslyondoyintv, n), (toke, v), (makinwa...</td>\n",
       "      <td>[previouslyondoyintv, toke, makinwaûªs, marri...</td>\n",
       "      <td>previouslyondoyintv toke makinwaûªs marriage ...</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8.111111</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>Check these out     nsfw</td>\n",
       "      <td>[check, these, out, nsfw]</td>\n",
       "      <td>[check, nsfw]</td>\n",
       "      <td>[(check, NN), (nsfw, NN)]</td>\n",
       "      <td>[(check, n), (nsfw, n)]</td>\n",
       "      <td>[check, nsfw]</td>\n",
       "      <td>check nsfw</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Los Angeles, Califnordia</td>\n",
       "      <td>PSA: IÛªm splitting my personalities.\\n\\n?? t...</td>\n",
       "      <td>PSA IÛªm splitting my personalities\\n\\n techi...</td>\n",
       "      <td>[psa, iûªm, splitting, my, personalities, tec...</td>\n",
       "      <td>[psa, iûªm, splitting, personalities, techies...</td>\n",
       "      <td>[(psa, NN), (iûªm, NN), (splitting, VBG), (pe...</td>\n",
       "      <td>[(psa, n), (iûªm, n), (splitting, v), (person...</td>\n",
       "      <td>[psa, iûªm, split, personality, techie, follo...</td>\n",
       "      <td>psa iûªm split personality techie follow abla...</td>\n",
       "      <td>85</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6.545455</td>\n",
       "      <td>6.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                        location  \\\n",
       "0    0     NaN                             NaN   \n",
       "1    2     NaN                             NaN   \n",
       "2    3     NaN                             NaN   \n",
       "3    9     NaN                             NaN   \n",
       "4   11     NaN                             NaN   \n",
       "5   12     NaN                             NaN   \n",
       "6   21     NaN                             NaN   \n",
       "7   22     NaN                             NaN   \n",
       "8   27     NaN                             NaN   \n",
       "9   29     NaN                             NaN   \n",
       "10  30     NaN                             NaN   \n",
       "11  35     NaN                             NaN   \n",
       "12  42     NaN                             NaN   \n",
       "13  43     NaN                             NaN   \n",
       "14  45     NaN                             NaN   \n",
       "15  46  ablaze                          London   \n",
       "16  47  ablaze  Niall's place | SAF 12 SQUAD |   \n",
       "17  51  ablaze                         NIGERIA   \n",
       "18  58  ablaze                  Live On Webcam   \n",
       "19  60  ablaze        Los Angeles, Califnordia   \n",
       "\n",
       "                                                 text  \\\n",
       "0                  Just happened a terrible car crash   \n",
       "1   Heard about #earthquake is different cities, s...   \n",
       "2   there is a forest fire at spot pond, geese are...   \n",
       "3            Apocalypse lighting. #Spokane #wildfires   \n",
       "4       Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "5                  We're shaking...It's an earthquake   \n",
       "6   They'd probably still show more life than Arse...   \n",
       "7                                   Hey! How are you?   \n",
       "8                                    What a nice hat?   \n",
       "9                                           Fuck off!   \n",
       "10                              No I don't like cold!   \n",
       "11                         NOOOOOOOOO! Don't do that!   \n",
       "12                             No don't tell me that!   \n",
       "13                                          What if?!   \n",
       "14                                           Awesome!   \n",
       "15  Birmingham Wholesale Market is ablaze BBC News...   \n",
       "16  @sunkxssedharry will you wear shorts for race ...   \n",
       "17  #PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...   \n",
       "18  Check these out: http://t.co/rOI2NSmEJJ http:/...   \n",
       "19  PSA: IÛªm splitting my personalities.\\n\\n?? t...   \n",
       "\n",
       "                                           text_clean  \\\n",
       "0                  Just happened a terrible car crash   \n",
       "1   Heard about earthquake is different cities sta...   \n",
       "2   there is a forest fire at spot pond geese are ...   \n",
       "3               Apocalypse lighting Spokane wildfires   \n",
       "4       Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "5                       Were shakingIts an earthquake   \n",
       "6   Theyd probably still show more life than Arsen...   \n",
       "7                                     Hey How are you   \n",
       "8                                     What a nice hat   \n",
       "9                                            Fuck off   \n",
       "10                                No I dont like cold   \n",
       "11                            NOOOOOOOOO Dont do that   \n",
       "12                               No dont tell me that   \n",
       "13                                            What if   \n",
       "14                                            Awesome   \n",
       "15  Birmingham Wholesale Market is ablaze BBC News...   \n",
       "16  sunkxssedharry will you wear shorts for race a...   \n",
       "17  PreviouslyOnDoyinTv Toke MakinwaÛªs marriage ...   \n",
       "18                           Check these out     nsfw   \n",
       "19  PSA IÛªm splitting my personalities\\n\\n techi...   \n",
       "\n",
       "                                           text_token  \\\n",
       "0           [just, happened, a, terrible, car, crash]   \n",
       "1   [heard, about, earthquake, is, different, citi...   \n",
       "2   [there, is, a, forest, fire, at, spot, pond, g...   \n",
       "3          [apocalypse, lighting, spokane, wildfires]   \n",
       "4   [typhoon, soudelor, kills, 28, in, china, and,...   \n",
       "5                  [were, shakingits, an, earthquake]   \n",
       "6   [theyd, probably, still, show, more, life, tha...   \n",
       "7                                [hey, how, are, you]   \n",
       "8                                [what, a, nice, hat]   \n",
       "9                                         [fuck, off]   \n",
       "10                          [no, i, dont, like, cold]   \n",
       "11                       [nooooooooo, dont, do, that]   \n",
       "12                         [no, dont, tell, me, that]   \n",
       "13                                         [what, if]   \n",
       "14                                          [awesome]   \n",
       "15  [birmingham, wholesale, market, is, ablaze, bb...   \n",
       "16  [sunkxssedharry, will, you, wear, shorts, for,...   \n",
       "17  [previouslyondoyintv, toke, makinwaûªs, marri...   \n",
       "18                          [check, these, out, nsfw]   \n",
       "19  [psa, iûªm, splitting, my, personalities, tec...   \n",
       "\n",
       "                                           text_final  \\\n",
       "0                    [happened, terrible, car, crash]   \n",
       "1   [heard, earthquake, different, cities, stay, s...   \n",
       "2   [forest, fire, spot, pond, geese, fleeing, acr...   \n",
       "3          [apocalypse, lighting, spokane, wildfires]   \n",
       "4       [typhoon, soudelor, kills, 28, china, taiwan]   \n",
       "5                            [shakingits, earthquake]   \n",
       "6   [theyd, probably, still, show, life, arsenal, ...   \n",
       "7                                               [hey]   \n",
       "8                                         [nice, hat]   \n",
       "9                                              [fuck]   \n",
       "10                                 [dont, like, cold]   \n",
       "11                                 [nooooooooo, dont]   \n",
       "12                                       [dont, tell]   \n",
       "13                                                 []   \n",
       "14                                          [awesome]   \n",
       "15  [birmingham, wholesale, market, ablaze, bbc, n...   \n",
       "16       [sunkxssedharry, wear, shorts, race, ablaze]   \n",
       "17  [previouslyondoyintv, toke, makinwaûªs, marri...   \n",
       "18                                      [check, nsfw]   \n",
       "19  [psa, iûªm, splitting, personalities, techies...   \n",
       "\n",
       "                                             pos_tags  \\\n",
       "0   [(happened, VBN), (terrible, JJ), (car, NN), (...   \n",
       "1   [(heard, RB), (earthquake, NN), (different, JJ...   \n",
       "2   [(forest, JJS), (fire, NN), (spot, NN), (pond,...   \n",
       "3   [(apocalypse, NN), (lighting, VBG), (spokane, ...   \n",
       "4   [(typhoon, NN), (soudelor, NN), (kills, VBZ), ...   \n",
       "5               [(shakingits, NNS), (earthquake, NN)]   \n",
       "6   [(theyd, NN), (probably, RB), (still, RB), (sh...   \n",
       "7                                         [(hey, NN)]   \n",
       "8                             [(nice, JJ), (hat, NN)]   \n",
       "9                                        [(fuck, NN)]   \n",
       "10               [(dont, NN), (like, IN), (cold, NN)]   \n",
       "11                    [(nooooooooo, NNS), (dont, NN)]   \n",
       "12                           [(dont, NN), (tell, NN)]   \n",
       "13                                                 []   \n",
       "14                                    [(awesome, NN)]   \n",
       "15  [(birmingham, JJ), (wholesale, JJ), (market, N...   \n",
       "16  [(sunkxssedharry, NN), (wear, NN), (shorts, NN...   \n",
       "17  [(previouslyondoyintv, NN), (toke, VBD), (maki...   \n",
       "18                          [(check, NN), (nsfw, NN)]   \n",
       "19  [(psa, NN), (iûªm, NN), (splitting, VBG), (pe...   \n",
       "\n",
       "                                          wordnet_tag  \\\n",
       "0   [(happened, v), (terrible, a), (car, n), (cras...   \n",
       "1   [(heard, r), (earthquake, n), (different, a), ...   \n",
       "2   [(forest, a), (fire, n), (spot, n), (pond, n),...   \n",
       "3   [(apocalypse, n), (lighting, v), (spokane, n),...   \n",
       "4   [(typhoon, n), (soudelor, n), (kills, v), (28,...   \n",
       "5                  [(shakingits, n), (earthquake, n)]   \n",
       "6   [(theyd, n), (probably, r), (still, r), (show,...   \n",
       "7                                          [(hey, n)]   \n",
       "8                               [(nice, a), (hat, n)]   \n",
       "9                                         [(fuck, n)]   \n",
       "10                  [(dont, n), (like, n), (cold, n)]   \n",
       "11                       [(nooooooooo, n), (dont, n)]   \n",
       "12                             [(dont, n), (tell, n)]   \n",
       "13                                                 []   \n",
       "14                                     [(awesome, n)]   \n",
       "15  [(birmingham, a), (wholesale, a), (market, n),...   \n",
       "16  [(sunkxssedharry, n), (wear, n), (shorts, n), ...   \n",
       "17  [(previouslyondoyintv, n), (toke, v), (makinwa...   \n",
       "18                            [(check, n), (nsfw, n)]   \n",
       "19  [(psa, n), (iûªm, n), (splitting, v), (person...   \n",
       "\n",
       "                                            lemmatize  \\\n",
       "0                      [happen, terrible, car, crash]   \n",
       "1   [heard, earthquake, different, city, stay, saf...   \n",
       "2   [forest, fire, spot, pond, geese, flee, across...   \n",
       "3              [apocalypse, light, spokane, wildfire]   \n",
       "4        [typhoon, soudelor, kill, 28, china, taiwan]   \n",
       "5                            [shakingits, earthquake]   \n",
       "6   [theyd, probably, still, show, life, arsenal, ...   \n",
       "7                                               [hey]   \n",
       "8                                         [nice, hat]   \n",
       "9                                              [fuck]   \n",
       "10                                 [dont, like, cold]   \n",
       "11                                 [nooooooooo, dont]   \n",
       "12                                       [dont, tell]   \n",
       "13                                                 []   \n",
       "14                                          [awesome]   \n",
       "15  [birmingham, wholesale, market, ablaze, bbc, n...   \n",
       "16        [sunkxssedharry, wear, short, race, ablaze]   \n",
       "17  [previouslyondoyintv, toke, makinwaûªs, marri...   \n",
       "18                                      [check, nsfw]   \n",
       "19  [psa, iûªm, split, personality, techie, follo...   \n",
       "\n",
       "                                           text_lemma  char_count1  \\\n",
       "0                           happen terrible car crash           34   \n",
       "1   heard earthquake different city stay safe ever...           61   \n",
       "2   forest fire spot pond geese flee across street...           94   \n",
       "3                   apocalypse light spokane wildfire           37   \n",
       "4               typhoon soudelor kill 28 china taiwan           45   \n",
       "5                               shakingits earthquake           29   \n",
       "6   theyd probably still show life arsenal yesterd...           68   \n",
       "7                                                 hey           15   \n",
       "8                                            nice hat           15   \n",
       "9                                                fuck            8   \n",
       "10                                     dont like cold           19   \n",
       "11                                    nooooooooo dont           23   \n",
       "12                                          dont tell           20   \n",
       "13                                                               7   \n",
       "14                                            awesome            7   \n",
       "15  birmingham wholesale market ablaze bbc news fi...           96   \n",
       "16              sunkxssedharry wear short race ablaze           52   \n",
       "17  previouslyondoyintv toke makinwaûªs marriage ...           82   \n",
       "18                                         check nsfw           24   \n",
       "19  psa iûªm split personality techie follow abla...           85   \n",
       "\n",
       "    char_count2  word_count1  word_count2  mword_leng1  mword_leng2  \n",
       "0            25            6            4     4.833333     5.500000  \n",
       "1            50            9            7     5.888889     6.285714  \n",
       "2            51           19            9     4.000000     4.777778  \n",
       "3            33            4            4     8.500000     7.500000  \n",
       "4            37            8            6     4.750000     5.333333  \n",
       "5            21            4            2     6.500000    10.000000  \n",
       "6            54           12            9     4.750000     5.111111  \n",
       "7             3            4            1     3.000000     3.000000  \n",
       "8             8            4            2     3.000000     3.500000  \n",
       "9             4            2            1     3.500000     4.000000  \n",
       "10           14            5            3     3.000000     4.000000  \n",
       "11           15            4            2     5.000000     7.000000  \n",
       "12            9            5            2     3.200000     4.000000  \n",
       "13            0            2            0     3.000000          NaN  \n",
       "14            7            1            1     7.000000     7.000000  \n",
       "15           83           14           11     5.785714     6.636364  \n",
       "16           37            8            5     5.500000     6.600000  \n",
       "17           80            9            9     8.111111     8.000000  \n",
       "18           10            4            2     4.250000     4.500000  \n",
       "19           71           11           10     6.545455     6.200000  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text_lemma'][13] = 'what if'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building Word embeddings Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(\"../data/preprocess_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def sent_vectorizer(sent):\n",
    "    doc = nlp(sent)\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for token in doc:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = token.vector\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, token.vector)\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.asarray(sent_vec)\n",
    "  \n",
    "# Saving the embeddings in a list X\n",
    "X_test=[]\n",
    "for sentence in df_test['text_lemma']:\n",
    "    X_test.append(sent_vectorizer(sentence))\n",
    "\n",
    "X_train=[]\n",
    "for sentence in df_train['text_lemma']:\n",
    "    X_train.append(sent_vectorizer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of X_train:  7613\n",
      "Number of X_test:  3263\n"
     ]
    }
   ],
   "source": [
    "print('Number of X_train: ', len(X_train))\n",
    "print('Number of X_test: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **_PCA for Dimension Reduction_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA for Dimensionality Reduction\n",
    "# And the StandardScaler to scale the data \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)\n",
    "pca_ = PCA(0.99, random_state=RSEED)\n",
    "X_pca_train=pca_.fit_transform(X_scaled_train)\n",
    "X_pca_test=pca_.transform(X_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.DataFrame(X_pca_train)\n",
    "features_test = pd.DataFrame(X_pca_test)\n",
    "#df_new = pd.concat([features, df['char_count2'], df_spacy['noun']], axis=1)\n",
    "df_new_train = pd.concat([features_train, df_train['char_count2']], axis=1)\n",
    "df_new_test = pd.concat([features_test, df_test['char_count2']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 249)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 249)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 250)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 250)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121634047024826\n"
     ]
    }
   ],
   "source": [
    "clf_ed_pca = LogisticRegression(random_state=RSEED, max_iter=500)\n",
    "\n",
    "clf_ed_pca.fit(df_new_train, df_train.target)\n",
    "accuracy = clf_ed_pca.score(df_new_train, df_train.target)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf_ed_pca.predict(df_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['target'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"../data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "0ed77dcdc7fbab6e38ae64c11601ec4b27d0c7090c2e616cfa30746b20795a2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
