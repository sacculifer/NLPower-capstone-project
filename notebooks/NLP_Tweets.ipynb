{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/grandhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis\n",
    "import regex as re\n",
    "import contractions\n",
    "import num2words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import ujson as json\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for making Data frames from .ndjson files\n",
    "\n",
    "def new_data(path=\"./data/incident-tweets/\", year = 2014, disaster = 'Biological', location = 'Germany'):\n",
    "    records1 = map(json.loads, open(path, encoding=\"utf8\"))\n",
    "    df = pd.DataFrame.from_records(records1)\n",
    "    df['location'] = location\n",
    "    df['disaster'] =  disaster\n",
    "    df['year'] = year\n",
    "    df = df.drop(['id'],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>location</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;HASHTAG&gt; ebola symptoms. early treatment mean...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thinking about telling leadership to make indi...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we know people are contracting the disease, an...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is &lt;USER&gt; nurse &lt;HASHTAG&gt; &lt;NUMBER&gt; is at ...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you asked questions about ebola â€” and we hav...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>&lt;HASHTAG&gt; italiandirectionerwantameetandgreeti...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>&lt;USER&gt; @clapbackanna skjkkj this gif gets me e...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>&lt;USER&gt; peter and max a fables novel by bill wi...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>merda, to chateada</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>thanks to our sponsor thomas avenue ceramics -...</td>\n",
       "      <td>Biological</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3448 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   relevance  location  \\\n",
       "0     <HASHTAG> ebola symptoms. early treatment mean...  Biological       NaN   \n",
       "1     thinking about telling leadership to make indi...  Biological       NaN   \n",
       "2     we know people are contracting the disease, an...  Biological       NaN   \n",
       "3     this is <USER> nurse <HASHTAG> <NUMBER> is at ...  Biological       NaN   \n",
       "4     you asked questions about ebola â€” and we hav...  Biological       NaN   \n",
       "...                                                 ...         ...       ...   \n",
       "3443  <HASHTAG> italiandirectionerwantameetandgreeti...  Biological       NaN   \n",
       "3444  <USER> @clapbackanna skjkkj this gif gets me e...  Biological       NaN   \n",
       "3445  <USER> peter and max a fables novel by bill wi...  Biological       NaN   \n",
       "3446                                 merda, to chateada  Biological       NaN   \n",
       "3447  thanks to our sponsor thomas avenue ceramics -...  Biological       NaN   \n",
       "\n",
       "      year  \n",
       "0     2014  \n",
       "1     2014  \n",
       "2     2014  \n",
       "3     2014  \n",
       "4     2014  \n",
       "...    ...  \n",
       "3443  2014  \n",
       "3444  2014  \n",
       "3445  2014  \n",
       "3446  2014  \n",
       "3447  2014  \n",
       "\n",
       "[3448 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cmath import nan\n",
    "\n",
    "\n",
    "df1 = new_data(path=\"../data/incident-tweets/biological-ebola-2014.ndjson\", year = 2014, disaster = 'Biological',location= nan)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = new_data(path=\"../data/incident-tweets/biological-ebola-2014.ndjson\", year = 2014, disaster = 'Biological', location= 'nan')\n",
    "df2 = new_data(path=\"../data/incident-tweets/biological-ebola-2014.ndjson\", year = 2014, disaster = 'Biologocal', location=nan)\n",
    "df3 = new_data(\"../data/incident-tweets/earthquake-bohol-2013.ndjson\",year=2013, disaster = 'Earthquake',location='bohol')\n",
    "df4 = new_data(\"../data/incident-tweets/earthquake-california-2013.ndjson\",year=2013, disaster = 'Earthquake',location='california')\n",
    "df5 = new_data(\"../data/incident-tweets/earthquake-chile-2013.ndjson\",year=2013, disaster = 'Earthquake',location='chile')\n",
    "df6 = new_data(\"../data/incident-tweets/earthquake-costarica-2012.ndjson\",year=2012, disaster = 'Earthquake',location='costarica')\n",
    "df7 = new_data(\"../data/incident-tweets/earthquake-guatemala-2012.ndjson\",year=2012, disaster = 'Earthquake', location='guatemala')\n",
    "df8 = new_data(\"../data/incident-tweets/earthquake-iraq-iran-2017.ndjson\",year=2017, disaster = 'Earthquake',location='iran')\n",
    "df9 = new_data(\"../data/incident-tweets/earthquake-italy-2012.ndjson\",year=2012, disaster = 'Earthquake', location='italy')\n",
    "df10 = new_data(\"../data/incident-tweets/earthquake-mexico-2017.ndjson\",year=2017, disaster = 'Earthquake',location='mexico')\n",
    "df11 = new_data(\"../data/incident-tweets/earthquake-nepal-2015.ndjson\",year=2015, disaster = 'Earthquake',location='nepal')\n",
    "df12 = new_data(\"../data/incident-tweets/earthquake-nepal-2018.ndjson\",year=2018, disaster = 'Earthquake',location='nepal')\n",
    "df13 = new_data(\"../data/incident-tweets/earthquake-pakistan-2013.ndjson\",year=2013, disaster = 'Earthquake',location='pakistan')\n",
    "df14 = new_data(\"../data/incident-tweets/flood-alberta-2013.ndjson\",year=2013, disaster = 'Flood',location='alberta')\n",
    "df15 = new_data(\"../data/incident-tweets/flood-colorado-2013.ndjson\",year=2013, disaster = 'Flood',location='colorado')\n",
    "df16 = new_data(\"../data/incident-tweets/flood-india-2014.ndjson\",year=2014, disaster = 'Flood', location='india')\n",
    "df17 = new_data(\"../data/incident-tweets/flood-manila-2013.ndjson\",year=2013, disaster = 'Flood',location='manila')\n",
    "df18 = new_data(\"../data/incident-tweets/flood-pakistan-2014.ndjson\",year=2014, disaster = 'Flood',location='pakistan')\n",
    "df19 = new_data(\"../data/incident-tweets/flood-philipinnes-2012.ndjson\",year=2012, disaster = 'Flood', location='philipinnes')\n",
    "df20 = new_data(\"../data/incident-tweets/flood-queensland-2013.ndjson\",year=2013, disaster = 'Flood',location='queensland')\n",
    "df21 = new_data(\"../data/incident-tweets/flood-sardinia-2013.ndjson\",year=2013, disaster = 'Flood', location='sardinia')\n",
    "df22 = new_data(\"../data/incident-tweets/flood-srilanka-2017.ndjson\",year=2017, disaster = 'Flood',location='srilanka')\n",
    "df23 = new_data(\"../data/incident-tweets/hurricane-hagupit-2014.ndjson\",year=2014, disaster = 'Hurricane',location='hagupit')\n",
    "df24 = new_data(\"../data/incident-tweets/hurricane-harvey-2017.ndjson\",year=2017, disaster = 'Hurricane', location='harvey')\n",
    "df25 = new_data(\"../data/incident-tweets/hurricane-irma-2017.ndjson\",year=2017, disaster = 'Hurricane', location='iram')\n",
    "df26 = new_data(\"../data/incident-tweets/hurricane-maria-2017.ndjson\",year=2017, disaster = 'Hurricane', location= 'maria')\n",
    "df27 = new_data(\"../data/incident-tweets/hurricane-pablo-2012.ndjson\",year=2012, disaster = 'Hurricane', location='pablo')\n",
    "df28 = new_data(\"../data/incident-tweets/hurricane-pam-2015.ndjson\",year=2015, disaster = 'Hurricane', location='pam')\n",
    "df29 = new_data(\"../data/incident-tweets/hurricane-sandy-2012.ndjson\",year=2012, disaster = 'Hurricane',location='sandy')\n",
    "df30 = new_data(\"../data/incident-tweets/hurricane-yolanda-2013.ndjson\",year=2013, disaster = 'Hurricane', location='yolanda')\n",
    "df31 = new_data(\"../data/incident-tweets/hurricane-odile-2014.ndjson\",year=2014, disaster = 'Hurricane',location='odile')\n",
    "df32 = new_data(\"../data/incident-tweets/industrial-savar-building-collapse-2013.ndjson\",year=2013, disaster = 'Industrial',location='savar')\n",
    "df33 = new_data(\"../data/incident-tweets/industrial-texas-explosion-2013.ndjson\",year=2013, disaster = 'Industrial',location='texas')\n",
    "df34 = new_data(\"../data/incident-tweets/industrial-venezuela-refinery-fire-2012.ndjson\",year=2012, disaster = 'Industrial',location='venezula')\n",
    "df35 = new_data(\"../data/incident-tweets/other-russia-meteor-2013.ndjson\",year=2013, disaster = 'Meteor',location='russia')\n",
    "df36 = new_data(\"../data/incident-tweets/other-singapore-haze-2013.ndjson\",year=2013, disaster = 'Haze',location='singapore')\n",
    "df37 = new_data(\"../data/incident-tweets/societal-boston-bombing-2013.ndjson\",year=2013, disaster = 'Societal',location='boston')\n",
    "df38 = new_data(\"../data/incident-tweets/societal-brazil-nightclub-fire-2013.ndjson\",year=2013, disaster = 'Societal',location='brazil')\n",
    "df39 = new_data(\"../data/incident-tweets/societal-la-airport-shooting-2013.ndjson\",year=2013, disaster = 'Societal',location='los angeles')\n",
    "df40 = new_data(\"../data/incident-tweets/tornado-joplin-2011.ndjson\",year=2011, disaster = 'Tornado', location='joplin')\n",
    "df41 = new_data(\"../data/incident-tweets/tornado-oklahoma-2013.ndjson\",year=2013, disaster = 'Tornado',location='oklahoma')\n",
    "df42 = new_data(\"../data/incident-tweets/transportation-glasgow-helicopter-crash-2013.ndjson\",year=2013, disaster = 'Transportation', location='glasgow')\n",
    "df43 = new_data(\"../data/incident-tweets/transportation-la-train-crash-2013.ndjson\",year=2013, disaster = 'Transportation',location='los angeles')\n",
    "df44 = new_data(\"../data/incident-tweets/transportation-ny-train-crash-2013.ndjson\",year=2013, disaster = 'Transportation',location='newyork')\n",
    "df45 = new_data(\"../data/incident-tweets/transportation-spain-train-crash-2013.ndjson\",year=2013, disaster = 'Transportation',location='spain')\n",
    "df46 = new_data(\"../data/incident-tweets/wildfire-australia-2013.ndjson\",year=2013, disaster = 'Wildfire',location='australia')\n",
    "df47 = new_data(\"../data/incident-tweets/wildfire-california-2014.ndjson\",year=2014, disaster = 'Wildfire',location='california')\n",
    "df48 = new_data(\"../data/incident-tweets/wildfire-colorado-2012.ndjson\",year=2011, disaster = 'Wildfire', location='colorado')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>location</th>\n",
       "      <th>disaster</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;HASHTAG&gt; ebola symptoms. early treatment mean...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thinking about telling leadership to make indi...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we know people are contracting the disease, an...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is &lt;USER&gt; nurse &lt;HASHTAG&gt; &lt;NUMBER&gt; is at ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you asked questions about ebola â€” and we hav...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>will it be easy? nope. will it be worth it? ab...</td>\n",
       "      <td>0</td>\n",
       "      <td>colorado</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>&lt;USER&gt; all is cool here thank god!</td>\n",
       "      <td>0</td>\n",
       "      <td>colorado</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>&lt;USER&gt; she's not adding up. she said she was &lt;...</td>\n",
       "      <td>0</td>\n",
       "      <td>colorado</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>just listening to dave days' covers/originals,...</td>\n",
       "      <td>0</td>\n",
       "      <td>colorado</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>\"anyone you're hoping to work with?\" demi: \"my...</td>\n",
       "      <td>0</td>\n",
       "      <td>colorado</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165732 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  relevance  location  \\\n",
       "0     <HASHTAG> ebola symptoms. early treatment mean...          1       nan   \n",
       "1     thinking about telling leadership to make indi...          1       nan   \n",
       "2     we know people are contracting the disease, an...          1       nan   \n",
       "3     this is <USER> nurse <HASHTAG> <NUMBER> is at ...          1       nan   \n",
       "4     you asked questions about ebola â€” and we hav...          1       nan   \n",
       "...                                                 ...        ...       ...   \n",
       "1797  will it be easy? nope. will it be worth it? ab...          0  colorado   \n",
       "1798                 <USER> all is cool here thank god!          0  colorado   \n",
       "1799  <USER> she's not adding up. she said she was <...          0  colorado   \n",
       "1800  just listening to dave days' covers/originals,...          0  colorado   \n",
       "1801  \"anyone you're hoping to work with?\" demi: \"my...          0  colorado   \n",
       "\n",
       "        disaster  year  \n",
       "0     Biological  2014  \n",
       "1     Biological  2014  \n",
       "2     Biological  2014  \n",
       "3     Biological  2014  \n",
       "4     Biological  2014  \n",
       "...          ...   ...  \n",
       "1797    Wildfire  2011  \n",
       "1798    Wildfire  2011  \n",
       "1799    Wildfire  2011  \n",
       "1800    Wildfire  2011  \n",
       "1801    Wildfire  2011  \n",
       "\n",
       "[165732 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,\n",
    "        df11,df12,df13,df14,df15,df16,df17,df17,df18,df19,df20,\n",
    "        df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,\n",
    "        df31,df32,df33,df34,df35,df36,df37,df38,df39,df40,\n",
    "        df41,df42,df43,df44,df45,df46,df47,df48]\n",
    "\n",
    "df_final = pd.concat(frames)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nepal          16522\n",
       "sandy          12530\n",
       "boston          9576\n",
       "alberta         9060\n",
       "texas           8102\n",
       "oklahoma        8056\n",
       "iram            7782\n",
       "harvey          7706\n",
       "maria           7674\n",
       "pakistan        6982\n",
       "queensland      6336\n",
       "california      6254\n",
       "hagupit         3916\n",
       "pam             3886\n",
       "chile           3840\n",
       "joplin          3752\n",
       "india           3594\n",
       "colorado        3508\n",
       "nan             3448\n",
       "los angeles     2578\n",
       "odile           2458\n",
       "manila          2448\n",
       "mexico          2396\n",
       "australia       1730\n",
       "glasgow         1554\n",
       "yolanda         1544\n",
       "russia          1524\n",
       "newyork         1498\n",
       "srilanka        1480\n",
       "savar           1462\n",
       "pablo           1426\n",
       "bohol           1330\n",
       "singapore       1328\n",
       "philipinnes     1324\n",
       "iran             966\n",
       "spain            704\n",
       "costarica        568\n",
       "brazil           478\n",
       "guatemala        390\n",
       "italy            268\n",
       "sardinia         182\n",
       "venezula         124\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.location.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_final.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url = re.compile(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "df['text_final'] = df['text'].apply(lambda x: remove_url(x))\n",
    "\n",
    "\n",
    "#Contractions\n",
    "df['text_final_1'] = df['text_final'].apply(lambda x: [contractions.fix(word) for word in x.split(' ')])\n",
    "\n",
    "#joining back the list of items into one string\n",
    "df['text_final_1'] = [' '.join(map(str, l)) for l in df['text_final_1']]\n",
    "\n",
    "# Noise Cleaning - spacing, special characters, lowercasing \n",
    "\n",
    "df['text_final_1'] = df['text_final_1'].str.lower()\n",
    "df['text_final_1'] = df['text_final_1'].apply(lambda x: re.sub(r'[^\\w\\d\\s\\']+', '', x))\n",
    "df['text_final_2'] = df['text_final_1'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "\n",
    "#nltk tokenization\n",
    "\n",
    "df['text_final_1'] = df['text_final_1'].apply(word_tokenize)\n",
    "df['text_final_2'] = df['text_final_2'].apply(word_tokenize)\n",
    "\n",
    "# remove stop words\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['text_final_1'] = df['text_final_1'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df['text_final_1'] = [' '.join(map(str, l)) for l in df['text_final_1']]\n",
    "\n",
    "df['text_final_2'] = df['text_final_2'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df['text_final_2'] = [' '.join(map(str, l)) for l in df['text_final_2']]\n",
    "\n",
    "\n",
    "# lemmatization\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "df['text_final_2'] = df['text_final_2'].apply(lambda x: [lemma.lemmatize(word) for word in x ])\n",
    "df['text_final_2'] = [''.join(map(str, l)) for l in df['text_final_2']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>location</th>\n",
       "      <th>disaster</th>\n",
       "      <th>year</th>\n",
       "      <th>text_final</th>\n",
       "      <th>text_final_1</th>\n",
       "      <th>text_final_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;HASHTAG&gt; ebola symptoms. early treatment mean...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "      <td>&lt;HASHTAG&gt; ebola symptoms. early treatment mean...</td>\n",
       "      <td>hashtag ebola symptoms early treatment means m...</td>\n",
       "      <td>hashtag ebola symptoms early treatment means m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thinking about telling leadership to make indi...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "      <td>thinking about telling leadership to make indi...</td>\n",
       "      <td>thinking telling leadership make individual si...</td>\n",
       "      <td>thinking telling leadership make individual si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we know people are contracting the disease, an...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "      <td>we know people are contracting the disease, an...</td>\n",
       "      <td>know people contracting disease dying without ...</td>\n",
       "      <td>know people contracting disease dying without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is &lt;USER&gt; nurse &lt;HASHTAG&gt; &lt;NUMBER&gt; is at ...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "      <td>this is &lt;USER&gt; nurse &lt;HASHTAG&gt; &lt;NUMBER&gt; is at ...</td>\n",
       "      <td>user nurse hashtag number moment rushed dallas...</td>\n",
       "      <td>user nurse hashtag number moment rushed dallas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you asked questions about ebola â€” and we hav...</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>Biological</td>\n",
       "      <td>2014</td>\n",
       "      <td>you asked questions about ebola â€” and we hav...</td>\n",
       "      <td>asked questions ebola â answers</td>\n",
       "      <td>asked questions ebola answers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  relevance location  \\\n",
       "0  <HASHTAG> ebola symptoms. early treatment mean...          1      nan   \n",
       "1  thinking about telling leadership to make indi...          1      nan   \n",
       "2  we know people are contracting the disease, an...          1      nan   \n",
       "3  this is <USER> nurse <HASHTAG> <NUMBER> is at ...          1      nan   \n",
       "4  you asked questions about ebola â€” and we hav...          1      nan   \n",
       "\n",
       "     disaster  year                                         text_final  \\\n",
       "0  Biological  2014  <HASHTAG> ebola symptoms. early treatment mean...   \n",
       "1  Biological  2014  thinking about telling leadership to make indi...   \n",
       "2  Biological  2014  we know people are contracting the disease, an...   \n",
       "3  Biological  2014  this is <USER> nurse <HASHTAG> <NUMBER> is at ...   \n",
       "4  Biological  2014  you asked questions about ebola â€” and we hav...   \n",
       "\n",
       "                                        text_final_1  \\\n",
       "0  hashtag ebola symptoms early treatment means m...   \n",
       "1  thinking telling leadership make individual si...   \n",
       "2  know people contracting disease dying without ...   \n",
       "3  user nurse hashtag number moment rushed dallas...   \n",
       "4                    asked questions ebola â answers   \n",
       "\n",
       "                                        text_final_2  \n",
       "0  hashtag ebola symptoms early treatment means m...  \n",
       "1  thinking telling leadership make individual si...  \n",
       "2  know people contracting disease dying without ...  \n",
       "3  user nurse hashtag number moment rushed dallas...  \n",
       "4                      asked questions ebola answers  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "from transformers import pipeline\n",
    "\n",
    "#https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion?text=I+like+you.+I+love+you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fear</td>\n",
       "      <td>0.440499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.722099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>0.281508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anger</td>\n",
       "      <td>0.586104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.795742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165727</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.934948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165728</th>\n",
       "      <td>joy</td>\n",
       "      <td>0.576523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165729</th>\n",
       "      <td>sadness</td>\n",
       "      <td>0.519028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165730</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.603765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165731</th>\n",
       "      <td>joy</td>\n",
       "      <td>0.427462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165732 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          label     score\n",
       "0          fear  0.440499\n",
       "1       neutral  0.722099\n",
       "2          fear  0.281508\n",
       "3         anger  0.586104\n",
       "4       neutral  0.795742\n",
       "...         ...       ...\n",
       "165727  neutral  0.934948\n",
       "165728      joy  0.576523\n",
       "165729  sadness  0.519028\n",
       "165730  neutral  0.603765\n",
       "165731      joy  0.427462\n",
       "\n",
       "[165732 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_pro = list(df['text_final_2'])\n",
    "\n",
    "output_new = classifier(tweets_pro)\n",
    "\n",
    "new_list_pro = pd.DataFrame(output_new)\n",
    "new_list_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.43692827224731445}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('thanks for your presence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     69067\n",
       "fear        26017\n",
       "joy         23180\n",
       "sadness     22933\n",
       "anger       13034\n",
       "surprise    10244\n",
       "disgust      1257\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list_pro.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de2b2c42f19cdb5d2f24461361f61e9d985b64a4ca4a8f91e8c83c5a49062fa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
